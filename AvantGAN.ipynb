{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AvantGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UOLarGaaXwX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "else:\n",
        "    device_name = 'CPU:0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vwg20CP-E1z"
      },
      "source": [
        "# Unzip data\n",
        "drawings_archive_path = '/content/avantgarde_drawings.zip'\n",
        "shutil.unpack_archive(drawings_archive_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQ61mmI_cYd"
      },
      "source": [
        "# Config class needed for DataManager and GAN classes\n",
        "class Config:\n",
        "    def from_json(config_path):\n",
        "        with open(config_path, \"r\") as config_file:\n",
        "          config = json.load(config_file)\n",
        "        return config"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9WNZxKvRiMz"
      },
      "source": [
        "# DataManager class for Image, Z-vec Data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL\n",
        "import json\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self, config_file):\n",
        "        config = Config.from_json(config_file)\n",
        "        self.image_dims = config['image_dims']\n",
        "        self.n_pnts = config['n_pnts']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.n_batches = config['n_pnts']//self.batch_size\n",
        "        self.z_size = config['z_size']\n",
        "        self.vec_mode = config['z_vec_mode']\n",
        "        self.data_dir = config['data_dir']\n",
        "\n",
        "        if self.vec_mode == 'uniform':\n",
        "            self.fixed_z = tf.random.uniform(\n",
        "                shape=(self.batch_size, self.z_size),\n",
        "                minval=-1, maxval=1)\n",
        "        elif self.vec_mode == 'normal':\n",
        "            self.fixed_z = tf.random.normal(\n",
        "                shape=(self.batch_size, self.z_size))\n",
        "\n",
        "    def prepare_data(self):\n",
        "        image_len, image_height = self.image_dims[0], self.image_dims[1]\n",
        "        image_size = image_len, image_height\n",
        "\n",
        "        train_datagen = ImageDataGenerator(preprocessing_function=DataManager.rescale)\n",
        "\n",
        "        shared_imgs = train_datagen.flow_from_directory(self.data_dir,\n",
        "                                                    target_size=image_size,\n",
        "                                                    batch_size=self.batch_size,\n",
        "                                                    shuffle=False)\n",
        "\n",
        "        disc_imgs = train_datagen.flow_from_directory(self.data_dir,\n",
        "                                                          target_size=image_size,\n",
        "                                                          batch_size=self.batch_size,\n",
        "                                                          shuffle=False)\n",
        "\n",
        "        shared_data = tf.data.Dataset.from_generator(\n",
        "            lambda: shared_imgs,\n",
        "            output_types=(tf.float32, tf.float32),\n",
        "            output_shapes = ([None,image_len,image_len,3],\n",
        "                            [None,1]))\n",
        "\n",
        "        disc_data = tf.data.Dataset.from_generator(\n",
        "            lambda: disc_imgs,\n",
        "            output_types=(tf.float32, tf.float32),\n",
        "            output_shapes = ([None,image_len,image_len,3],\n",
        "                            [None,1]))\n",
        "\n",
        "        input_zs = self.get_noise_vecs()\n",
        "        shared_data_iter = list(tf.data.Dataset.zip((input_zs, shared_data)).as_numpy_iterator())\n",
        "        disc_data_iter = list(tf.data.Dataset.zip((input_zs, disc_data)).as_numpy_iterator())\n",
        "\n",
        "        # shuffle data\n",
        "        np.random.shuffle(shared_data_iter)\n",
        "        np.random.shuffle(disc_data_iter)\n",
        "\n",
        "        self.shared_data_iter = shared_data_iter\n",
        "        self.disc_data_iter = disc_data_iter\n",
        "\n",
        "\n",
        "    def rescale(image):\n",
        "        \"puts color values in [-1,1] range\"\n",
        "        return (image/255.)*2.-1\n",
        "\n",
        "    def get_noise_vector(self):\n",
        "        'noise vector for the generator input'\n",
        "        if self.vec_mode == 'uniform':\n",
        "            input_z = tf.random.uniform(\n",
        "                shape=(self.z_size,), minval=-1.0, maxval=1.0)\n",
        "        elif self.vec_mode == 'normal':\n",
        "            input_z = tf.random.normal(shape=(self.z_size,))\n",
        "        return input_z\n",
        "\n",
        "    def get_noise_vecs(self):\n",
        "        X = list(range(self.n_pnts))\n",
        "        input_zs = tf.data.Dataset.from_tensor_slices(X)\n",
        "        input_zs = input_zs.map(lambda x: self.get_noise_vector())\n",
        "        input_zs = input_zs.batch(self.batch_size, drop_remainder=False)\n",
        "        return input_zs\n",
        "\n",
        "    def is_aligned(data):\n",
        "        \"makes sure each image has a z_vector in each batch\"\n",
        "        for batch_i, (z_vecs, (imgs,_)) in enumerate(data):\n",
        "          if len(z_vecs) != len(imgs):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def shuffle_shared_data(self):\n",
        "        np.random.shuffle(self.shared_data_iter)\n",
        "\n",
        "    def get_shared_data_iter(self):\n",
        "        return self.shared_data_iter\n",
        "\n",
        "    def get_disc_data(self, n_pnt_to_retrieve):\n",
        "        np.random.shuffle(self.disc_data_iter)\n",
        "        for i in range(n_pnt_to_retrieve):\n",
        "            yield self.disc_data_iter[i]\n",
        "\n",
        "    def get_fixed_z(self):\n",
        "        return self.fixed_z\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size\n",
        "\n",
        "    def is_data_valid(self):\n",
        "        return DataManager.is_aligned(self.disc_data_iter) and \\\n",
        "              DataManager.is_aligned(self.shared_data_iter)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAV2x4pq-zyy"
      },
      "source": [
        "# Classes for both GAN networks\n",
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "\n",
        "class BaseModel(ABC):\n",
        "    \"\"\"Abstract Model class\"\"\"\n",
        "    def __init__(self, config_file):\n",
        "          self.config = Config.from_json(config_file)\n",
        "          self.config['size_factor'] = 2**self.config['n_blocks']\n",
        "          self.config['hidden']['size'] = (\n",
        "            self.config['img_size'][0]//self.config['size_factor'], \n",
        "            self.config['img_size'][1]//self.config['size_factor']\n",
        "          )\n",
        "\n",
        "    @abstractmethod\n",
        "    def build(self):\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_output_layer(self):\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_model_with_base_layers(self):\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self):\n",
        "      pass\n",
        "\n",
        "class GeneratorDCGAN(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config['optimizer']['learning_rate'],\n",
        "            beta_1=self.config['optimizer']['beta_1'],\n",
        "            beta_2=self.config['optimizer']['beta_2'])\n",
        "\n",
        "    def set_discriminator(self, disc_model):\n",
        "        self.disc_model = disc_model\n",
        "\n",
        "    def get_discriminator(self):\n",
        "        return self.disc_model\n",
        "\n",
        "    def add_output_layer(self):         \n",
        "        self.model.add(\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=self.config['img_size'][2],\n",
        "                kernel_size=self.config['output']['kernel_size'], \n",
        "                strides=self.config['output']['strides'],\n",
        "                padding=self.config['output']['padding'],\n",
        "                use_bias=self.config['output']['use_bias'], \n",
        "                activation=self.config['output']['activation']))\n",
        "    \n",
        "    def build_model_with_base_layers(self):\n",
        "        partial_model = tf.keras.Sequential([\n",
        "                  tf.keras.layers.Input(shape=(self.config['z_size'])),\n",
        "                  tf.keras.layers.Dense(\n",
        "                      units=self.config['n_filters'] \\\n",
        "                      *np.prod(self.config['hidden']['size']), \n",
        "                      use_bias=self.config['base']['dense']['use_bias']),\n",
        "                  tf.keras.layers.BatchNormalization(),\n",
        "                  tf.keras.layers.LeakyReLU(),\n",
        "                  tf.keras.layers.Reshape(\n",
        "                      (self.config['hidden']['size'][0],\n",
        "                      self.config['hidden']['size'][1],\n",
        "                      self.config['n_filters'])),\n",
        "                  tf.keras.layers.Conv2DTranspose(\n",
        "                      filters=self.config['n_filters'],\n",
        "                      kernel_size=self.config['base']['tconv']['kernel_size'],\n",
        "                      strides=self.config['base']['tconv']['strides'],\n",
        "                      padding=self.config['base']['tconv']['padding'],\n",
        "                      use_bias=self.config['base']['tconv']['use_bias']),\n",
        "                  tf.keras.layers.BatchNormalization(),\n",
        "                  tf.keras.layers.LeakyReLU()\n",
        "              ])\n",
        "        self.model = partial_model\n",
        "\n",
        "    def add_hidden_blocks(self):\n",
        "        curr_n_filters = self.config['n_filters']\n",
        "        for i in range(self.config['n_blocks']):\n",
        "            curr_n_filters = curr_n_filters // 2\n",
        "            self.model.add(\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "              filters=curr_n_filters,\n",
        "              kernel_size=self.config['hidden']['kernel_size'],\n",
        "              strides=self.config['hidden']['strides'],\n",
        "              padding=self.config['hidden']['padding'],\n",
        "              use_bias=self.config['hidden']['use_bias']))\n",
        "            self.model.add(tf.keras.layers.BatchNormalization())\n",
        "            self.model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    def build(self):\n",
        "        self.build_model_with_base_layers()\n",
        "        self.add_hidden_blocks()\n",
        "        self.add_output_layer()\n",
        "\n",
        "    def train(self, input_vec):\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_img = self.__train(input_vec)\n",
        "            disc_pred = self.get_discriminator().predict(fake_img)\n",
        "            loss = -tf.math.reduce_mean(disc_pred)\n",
        "        \n",
        "        grads = tape.gradient(loss, gen_model.get_trainable_variables())\n",
        "        gen_model.get_optimizer().apply_gradients(zip(grads, gen_model.get_trainable_variables()))\n",
        "        return loss\n",
        "\n",
        "    def __train(self, input_vec):\n",
        "        return self.model(input_vec, training=True)\n",
        "\n",
        "    def apply_gradients(self, grads):\n",
        "        self.get_optimizer().apply_gradients(\n",
        "          grads_and_vars=zip(grads, self.model.trainable_variables))\n",
        "        \n",
        "    def generate(self, input_vec):\n",
        "        img = self.model(input_vec, training=False)\n",
        "        return img\n",
        "\n",
        "    def get_trainable_variables(self):\n",
        "        return self.model.trainable_variables\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return self.optimizer\n",
        "\n",
        "    def create_samples(self, input_z, batch_size):\n",
        "        fake_imgs = self.generate(input_z)\n",
        "        images = tf.reshape(fake_imgs, (batch_size, *self.config['img_size']))    \n",
        "        # convert scale from [-1,1] back to [0,1]\n",
        "        return (images+1)/2.0\n",
        "\n",
        "class DiscriminatorDCGAN(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config['optimizer']['learning_rate'],\n",
        "            beta_1=self.config['optimizer']['beta_1'],\n",
        "            beta_2=self.config['optimizer']['beta_2'])\n",
        "\n",
        "    def set_generator(self, gen_model):\n",
        "        self.gen_model = gen_model\n",
        "\n",
        "    def get_generator(self):\n",
        "        return self.gen_model\n",
        "\n",
        "    def build(self):\n",
        "        self.build_model_with_base_layers()\n",
        "        self.add_hidden_blocks()\n",
        "        self.add_output_layer()\n",
        "\n",
        "    def build_model_with_base_layers(self):\n",
        "        partial_model = tf.keras.Sequential([\n",
        "                  tf.keras.layers.Input(shape=(self.config['img_size'])),\n",
        "                  tf.keras.layers.Conv2D(\n",
        "                      filters=self.config['base']['conv']['n_filters'],\n",
        "                      kernel_size=self.config['base']['conv']['kernel_size'], \n",
        "                      strides=self.config['base']['conv']['strides'],\n",
        "                      padding=self.config['base']['conv']['padding']),\n",
        "                  tf.keras.layers.BatchNormalization(),\n",
        "                  tf.keras.layers.LeakyReLU()\n",
        "              ])\n",
        "        self.model = partial_model\n",
        "\n",
        "    def add_hidden_blocks(self):\n",
        "        curr_n_filters = self.config['n_filters']\n",
        "        for i in range(self.config['n_blocks']):\n",
        "            curr_n_filters = curr_n_filters * 2\n",
        "            self.model.add(\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=curr_n_filters,\n",
        "                    kernel_size=self.config['hidden']['kernel_size'], \n",
        "                    strides=self.config['hidden']['strides'],\n",
        "                    padding=self.config['hidden']['padding']))\n",
        "            self.model.add(tf.keras.layers.BatchNormalization())\n",
        "            self.model.add(tf.keras.layers.LeakyReLU())\n",
        "            self.model.add(tf.keras.layers.\n",
        "                           Dropout(self.config['hidden']\n",
        "                                              ['dropout_rate']))\n",
        "\n",
        "    def add_output_layer(self):\n",
        "        self.model.add(tf.keras.layers.Conv2D(\n",
        "          filters=self.config['output']['n_filters'],\n",
        "          kernel_size=self.config['hidden']['size'],\n",
        "          padding=self.config['output']['padding']))        \n",
        "        self.model.add(tf.keras.layers.Reshape((1,)))\n",
        "\n",
        "    def train(self, real_img, input_z):\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_img = self.get_generator().generate(input_z)\n",
        "            pred_real = disc_model.__train(real_img)\n",
        "            pred_fake = disc_model.__train(fake_img)\n",
        "\n",
        "            # Compute losses\n",
        "            loss_real = -tf.math.reduce_mean(pred_real)\n",
        "            loss_fake =  tf.math.reduce_mean(pred_fake)\n",
        "            tot_loss = loss_real + loss_fake\n",
        "\n",
        "            # Calculate gradient penalty\n",
        "            with tf.GradientTape() as gp_tape:\n",
        "                alpha = tf.random.uniform(\n",
        "                    shape=[pred_real.shape[0], 1, 1, 1], \n",
        "                    minval=0, maxval=1.0)\n",
        "                interpolated = (alpha*real_img + (1-alpha)*fake_img)\n",
        "                gp_tape.watch(interpolated)\n",
        "                critic_intp = disc_model.predict(interpolated)\n",
        "\n",
        "            grads_intp = gp_tape.gradient(\n",
        "                critic_intp, [interpolated,])[0]\n",
        "            grads_intp_l2 = tf.sqrt(\n",
        "                tf.reduce_sum(tf.square(grads_intp), axis=[1, 2, 3]))\n",
        "            grad_penalty = tf.reduce_mean(tf.square(grads_intp_l2 - 1.0))\n",
        "\n",
        "            # Apply gradient penalty\n",
        "            tot_loss += tot_loss + self.config['lambda_gp']*grad_penalty\n",
        "\n",
        "        # Compute and apply the gradients\n",
        "        grads = tape.gradient(tot_loss, disc_model.get_trainable_variables())\n",
        "        self.get_optimizer().apply_gradients(\n",
        "            grads_and_vars=zip(grads, disc_model.get_trainable_variables()))\n",
        "        \n",
        "        return tot_loss, loss_real, loss_fake\n",
        "\n",
        "    def __train(self, input_img):\n",
        "        return self.model(input_img, training=True)\n",
        "\n",
        "    def apply_gradients(self, grads):\n",
        "        self.get_optimizer().apply_gradients(\n",
        "          grads_and_vars=zip(grads, self.model.trainable_variables))\n",
        "        \n",
        "    def predict(self, input_img):\n",
        "        verdict = self.model(input_img, training=False)\n",
        "        return verdict\n",
        "\n",
        "    def get_trainable_variables(self):\n",
        "        return self.model.trainable_variables\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return self.optimizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90wUJKMChsZo"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set up data\n",
        "    data_manager = DataManager('data_config.json')\n",
        "    data_manager.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QLfDbJnVyV9"
      },
      "source": [
        "    # Set up GAN architecture\n",
        "    with tf.device(device_name):\n",
        "        gen_model = GeneratorDCGAN('generator_config.json')\n",
        "        gen_model.build()\n",
        "\n",
        "        disc_model = DiscriminatorDCGAN('discriminator_config.json')\n",
        "        disc_model.build()\n",
        "\n",
        "        gen_model.set_discriminator(disc_model)\n",
        "        disc_model.set_generator(gen_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfKlzlbcJsSD"
      },
      "source": [
        "    all_losses = []\n",
        "    epoch_samples = []\n",
        "    critic_rounds_per_generator = 5\n",
        "    n_epochs = 200\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        epoch_losses = []\n",
        "        data_manager.shuffle_shared_data()\n",
        "        for batch_i, (z_vec, (real_img, _)) in enumerate(data_manager.get_shared_data_iter()):\n",
        "            if (batch_i % 50) == 0:\n",
        "                print(\"batch\", batch_i)\n",
        "            # only train the discriminator\n",
        "            for bonus_z_vec, (bonus_real_img, _) in data_manager.get_disc_data(critic_rounds_per_generator):\n",
        "                disc_model.train(bonus_real_img, bonus_z_vec)\n",
        "            # train both discriminator and generator\n",
        "            d_loss, d_loss_real, d_loss_fake = disc_model.train(real_img, z_vec)\n",
        "            g_loss = gen_model.train(z_vec)\n",
        "\n",
        "            epoch_losses.append(\n",
        "                (g_loss.numpy(), d_loss.numpy(), \n",
        "                  d_loss_real.numpy(), d_loss_fake.numpy()))\n",
        "                            \n",
        "            all_losses.append(epoch_losses)\n",
        "\n",
        "        print('Epoch {:-3d} | Est. Time {:.2f} min | Avg Losses >>'\n",
        "              ' G/D {:6.2f}/{:6.2f} [D-Real: {:6.2f} D-Fake: {:6.2f}]'\n",
        "              .format(epoch, (time.time() - start_time)/60, \n",
        "                      *list(np.mean(all_losses[-1], axis=0)))\n",
        "        )\n",
        "\n",
        "        epoch_samples.append(\n",
        "            gen_model.create_samples(data_manager.get_fixed_z(),\n",
        "                                    data_manager.get_batch_size()).numpy()\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}